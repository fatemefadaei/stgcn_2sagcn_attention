{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDPTa-kcSVF1",
        "outputId": "bb586975-d802-4c58-c53c-9cfb0201f754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "VQJrMIIWSoXa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Use CUDA:', torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONkMTQ9aSqZq",
        "outputId": "4cb78e45-a284-42d4-cff9-9649c5acea23"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use CUDA: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.use_deterministic_algorithms = True"
      ],
      "metadata": {
        "id": "eMIyizmwSsSS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_instance(ins):\n",
        "  _,size,_,_=ins.shape\n",
        "  return ins.reshape(3,size,25)\n"
      ],
      "metadata": {
        "id": "gBt9Ozb6SuV6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_clip(data , length=300):\n",
        "  for item_iter in range(len(data)):\n",
        "    if data[item_iter].shape[1]>length:\n",
        "      data[item_iter]= data[item_iter][:,:length,:]\n",
        "  return data"
      ],
      "metadata": {
        "id": "I091s8KLSwpx"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Dataset pickles"
      ],
      "metadata": {
        "id": "FiwA8CzoS1Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://mprg.cs.chubu.ac.jp/~itaya/share/mprg_colab/NTU-RGBD_data/data.zip\n",
        "!unzip -q -o data.zip"
      ],
      "metadata": {
        "id": "4JnYhdl_S2bJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/Park/train_3d.pkl', 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "    # train_label = [int(i['label']/10) for i in train_data]\n",
        "    train_label = [i['label'] for i in train_data]\n",
        "    print(train_label[:10])\n",
        "    train_data = [reshape_instance(i['keypoint']) for i in train_data]\n",
        "\n",
        "with open('/content/drive/MyDrive/Park/test_3d.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "    # test_label = [int(i['label']/10) for i in test_data]\n",
        "    test_label = [i['label'] for i in test_data]\n",
        "    print(test_label[:10])\n",
        "    test_data = [reshape_instance(i['keypoint']) for i in test_data]\n",
        "\n",
        "\n",
        "with open('/content/drive/MyDrive/Park/val_3d.pkl', 'rb') as f:\n",
        "    val_data = pickle.load(f)\n",
        "    # val_label = [int(i['label']/10) for i in val_data]\n",
        "    val_label = [i['label'] for i in val_data]\n",
        "    print(val_label[:10])\n",
        "    val_data = [reshape_instance(i['keypoint']) for i in val_data]\n",
        "\n",
        "train_data = crop_clip(train_data)\n",
        "test_data = crop_clip(test_data)\n",
        "val_data = crop_clip(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc6OAaWiS-Pp",
        "outputId": "aa622c8d-11cf-40e9-aa3b-9592ab5a77b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71, 24, 87, 93, 69, 51, 21, 79, 36, 79]\n",
            "[21, 63, 87, 1, 2, 59, 30, 16, 8, 26]\n",
            "[5, 9, 52, 93, 4, 71, 59, 13, 7, 87]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(train_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4DTQ_o1TBN6",
        "outputId": "447af5ef-b5ce-44fb-8275-47b03d808de1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Defining collator and feeder"
      ],
      "metadata": {
        "id": "dAtm8-jhTGIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class MyCollator(object):\n",
        "    def __init__(self,test=False,percentile=4750):\n",
        "        self.test = test\n",
        "        self.percentile = percentile\n",
        "    def __call__(self, batch):\n",
        "        # data = [torch.Tensor(item[0]) for item in batch]\n",
        "\n",
        "\n",
        "\n",
        "        # data = pad_sequence(data, padding_value=0)\n",
        "        # target = pad_sequence(target, padding_value=0)\n",
        "        # target = torch.Tensor(target)\n",
        "        # for items in batch:\n",
        "        #   if items[0].shape[1]>300:\n",
        "        #     items[0]= items[0][:,:300,:,:]\n",
        "        max_len = 300\n",
        "        max_len2 = max([x[0].shape[2] for x in batch])\n",
        "\n",
        "        # return [data, target]\n",
        "        batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
        "        target = [item[1] for item in batch]\n",
        "        data = [torch.Tensor(item[0]) for item in batch]\n",
        "        # Pad the sequences to the length of the longest sequence in the batch\n",
        "        # padded_batch = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "        padded_batch = [torch.nn.functional.pad(torch.Tensor(gif[0]), ( 0,0,max_len - gif[0].shape[1], 0)) for gif in batch]\n",
        "        # stack = [[seq[i],label[i]] for i in range(padded_batch.shape[0])]\n",
        "\n",
        "        return [torch.stack(padded_batch),target]\n",
        "\n",
        "class MyCollator_backup(object):\n",
        "    def __init__(self,test=False,percentile=4750):\n",
        "        self.test = test\n",
        "        self.percentile = percentile\n",
        "    def __call__(self, batch):\n",
        "        # data = [torch.Tensor(item[0]) for item in batch]\n",
        "\n",
        "\n",
        "\n",
        "        # data = pad_sequence(data, padding_value=0)\n",
        "        # target = pad_sequence(target, padding_value=0)\n",
        "        # target = torch.Tensor(target)\n",
        "        max_len = max([x[0].shape[1] for x in batch])\n",
        "        max_len2 = max([x[0].shape[2] for x in batch])\n",
        "\n",
        "        # return [data, target]\n",
        "        batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
        "        target = [item[1] for item in batch]\n",
        "        data = [torch.Tensor(item[0]) for item in batch]\n",
        "        # Pad the sequences to the length of the longest sequence in the batch\n",
        "        # padded_batch = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "        padded_batch = [torch.nn.functional.pad(torch.Tensor(gif[0]), ( 0,0,max_len - gif[0].shape[1], 0)) for gif in batch]\n",
        "        # stack = [[seq[i],label[i]] for i in range(padded_batch.shape[0])]\n",
        "\n",
        "        return [torch.stack(padded_batch),target]\n",
        "\n",
        "# def pad_collate(batch):\n",
        "#   data = [item[0] for item in batch]\n",
        "#         target = [item[1] for item in batch]\n",
        "#         target = torch.LongTensor(target)\n",
        "\n",
        "#   xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
        "#   yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
        "\n",
        "#   return xx_pad, yy_pad, x_lens, y_lens\n"
      ],
      "metadata": {
        "id": "J2zDKI3sTKQi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Feeder(torch.utils.data.Dataset):\n",
        "  def __init__(self, data, label):\n",
        "      super().__init__()\n",
        "      self.label = label\n",
        "      self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.label)\n",
        "\n",
        "  def __iter__(self):\n",
        "      return self\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      data = np.array(self.data[index])\n",
        "      label = self.label[index]\n",
        "\n",
        "      return data, label"
      ],
      "metadata": {
        "id": "JYTgZwtzTM96"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining Graph to load data in the way ntu+rgb D models give it to action recongnition model"
      ],
      "metadata": {
        "id": "ONm8WUfSTP9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Graph():\n",
        "  def __init__(self, hop_size, strategy):\n",
        "    self.get_edge()\n",
        "\n",
        "    self.hop_size = hop_size\n",
        "    self.hop_dis = self.get_hop_distance(self.num_node, self.edge, hop_size=hop_size)\n",
        "\n",
        "    self.get_adjacency(strategy)\n",
        "\n",
        "  def __str__(self):\n",
        "    return self.A\n",
        "\n",
        "  def get_edge(self):\n",
        "    self.num_node = 25\n",
        "    self_link = [(i, i) for i in range(self.num_node)]\n",
        "    neighbor_base = [(1, 2), (2, 21), (3, 21), (4, 3), (5, 21),\n",
        "                      (6, 5), (7, 6), (8, 7), (9, 21), (10, 9),\n",
        "                      (11, 10), (12, 11), (13, 1), (14, 13), (15, 14),\n",
        "                      (16, 15), (17, 1), (18, 17), (19, 18), (20, 19),\n",
        "                      (22, 23), (23, 8), (24, 25), (25, 12)]\n",
        "    neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_base]\n",
        "    self.self_link = self_link\n",
        "    self.neighbor_link = neighbor_link\n",
        "    self.edge = self_link + neighbor_link\n",
        "    self.center = 21 - 1\n",
        "\n",
        "  def get_adjacency(self, strategy):\n",
        "    valid_hop = range(0, self.hop_size + 1, 1)\n",
        "    adjacency = np.zeros((self.num_node, self.num_node))\n",
        "    for hop in valid_hop:\n",
        "        adjacency[self.hop_dis == hop] = 1\n",
        "    normalize_adjacency = self.normalize_digraph(adjacency)\n",
        "    if strategy == 'spatial':\n",
        "      A = []\n",
        "      for hop in valid_hop:\n",
        "          a_root = np.zeros((self.num_node, self.num_node))\n",
        "          a_close = np.zeros((self.num_node, self.num_node))\n",
        "          a_further = np.zeros((self.num_node, self.num_node))\n",
        "          for i in range(self.num_node):\n",
        "              for j in range(self.num_node):\n",
        "                  if self.hop_dis[j, i] == hop:\n",
        "                      if self.hop_dis[j, self.center] == self.hop_dis[\n",
        "                              i, self.center]:\n",
        "                          a_root[j, i] = normalize_adjacency[j, i]\n",
        "                      elif self.hop_dis[j, self.center] > self.hop_dis[\n",
        "                              i, self.center]:\n",
        "                          a_close[j, i] = normalize_adjacency[j, i]\n",
        "                      else:\n",
        "                          a_further[j, i] = normalize_adjacency[j, i]\n",
        "          if hop == 0:\n",
        "              A.append(a_root)\n",
        "          else:\n",
        "              A.append(a_root + a_close)\n",
        "              A.append(a_further)\n",
        "      A = np.stack(A)\n",
        "      self.A = A\n",
        "    elif strategy == 'agcn':\n",
        "      A = []\n",
        "      link_mat = self.edge2mat(self.self_link, self.num_node)\n",
        "      In = self.normalize_digraph(self.edge2mat(self.neighbor_link, self.num_node))\n",
        "      outward = [(j, i) for (i, j) in self.neighbor_link]\n",
        "      Out = self.normalize_digraph(self.edge2mat(outward, self.num_node))\n",
        "      A = np.stack((link_mat, In, Out))\n",
        "      self.A = A\n",
        "    else:\n",
        "        raise ValueError('Do Not Exist This Strategy')\n",
        "\n",
        "  def get_hop_distance(self, num_node, edge, hop_size):\n",
        "    A = np.zeros((num_node, num_node))\n",
        "    for i, j in edge:\n",
        "        A[j, i] = 1\n",
        "        A[i, j] = 1\n",
        "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
        "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(hop_size + 1)]\n",
        "    arrive_mat = (np.stack(transfer_mat) > 0)\n",
        "    for d in range(hop_size, -1, -1):\n",
        "        hop_dis[arrive_mat[d]] = d\n",
        "    return hop_dis\n",
        "\n",
        "  def normalize_digraph(self, A):\n",
        "    Dl = np.sum(A, 0)\n",
        "    num_node = A.shape[0]\n",
        "    Dn = np.zeros((num_node, num_node))\n",
        "    for i in range(num_node):\n",
        "        if Dl[i] > 0:\n",
        "            Dn[i, i] = Dl[i]**(-1)\n",
        "    DAD = np.dot(A, Dn)\n",
        "    return DAD\n",
        "\n",
        "  def edge2mat(self, link, num_node):\n",
        "    A = np.zeros((num_node, num_node))\n",
        "    for i, j in link:\n",
        "        A[j, i] = 1\n",
        "    return A"
      ],
      "metadata": {
        "id": "lW8yuzb6TWAx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention Module class"
      ],
      "metadata": {
        "id": "OLxuk0GpTfW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_channels, in_channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_channels // reduction, in_channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y"
      ],
      "metadata": {
        "id": "UI30Y7OeThIz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AGCN Backbone"
      ],
      "metadata": {
        "id": "xUfrKoPYTsV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def kaiming_init(module: nn.Module,\n",
        "                 a: float = 0,\n",
        "                 mode: str = 'fan_out',\n",
        "                 nonlinearity: str = 'relu',\n",
        "                 bias: float = 0,\n",
        "                 distribution: str = 'normal') -> None:\n",
        "    assert distribution in ['uniform', 'normal']\n",
        "    if hasattr(module, 'weight') and module.weight is not None:\n",
        "        if distribution == 'uniform':\n",
        "            nn.init.kaiming_uniform_(\n",
        "                module.weight, a=a, mode=mode, nonlinearity=nonlinearity)\n",
        "        else:\n",
        "            nn.init.kaiming_normal_(\n",
        "                module.weight, a=a, mode=mode, nonlinearity=nonlinearity)\n",
        "    if hasattr(module, 'bias') and module.bias is not None:\n",
        "        nn.init.constant_(module.bias, bias)\n",
        "\n",
        "\n",
        "def constant_init(module: nn.Module, val: float, bias: float = 0) -> None:\n",
        "    if hasattr(module, 'weight') and module.weight is not None:\n",
        "        nn.init.constant_(module.weight, val)\n",
        "    if hasattr(module, 'bias') and module.bias is not None:\n",
        "        nn.init.constant_(module.bias, bias)\n",
        "\n",
        "\n",
        "def normal_init(module: nn.Module,\n",
        "                mean: float = 0,\n",
        "                std: float = 1,\n",
        "                bias: float = 0) -> None:\n",
        "    if hasattr(module, 'weight') and module.weight is not None:\n",
        "        nn.init.normal_(module.weight, mean, std)\n",
        "    if hasattr(module, 'bias') and module.bias is not None:\n",
        "        nn.init.constant_(module.bias, bias)\n",
        "\n",
        "\n",
        "\n",
        "def conv_branch_init(conv, branches):\n",
        "    weight = conv.weight\n",
        "    n = weight.size(0)\n",
        "    k1 = weight.size(1)\n",
        "    k2 = weight.size(2)\n",
        "    normal_init(weight, mean=0, std=math.sqrt(2. / (n * k1 * k2 * branches)))\n",
        "    constant_init(conv.bias, 0)\n",
        "\n",
        "def conv_init(conv):\n",
        "    kaiming_init(conv.weight)\n",
        "    constant_init(conv.bias, 0)\n",
        "\n",
        "def bn_init(bn, scale):\n",
        "    constant_init(bn.weight, scale)\n",
        "    constant_init(bn.bias, 0)\n",
        "\n",
        "\n",
        "def zero(x):\n",
        "    \"\"\"return zero.\"\"\"\n",
        "    return 0\n",
        "\n",
        "\n",
        "def identity(x):\n",
        "    \"\"\"return input itself.\"\"\"\n",
        "    return x"
      ],
      "metadata": {
        "id": "wnRnSOwlUBOK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvTemporalGraphical(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 t_kernel_size=1,\n",
        "                 t_stride=1,\n",
        "                 t_padding=0,\n",
        "                 t_dilation=1,\n",
        "                 adj_len=25,\n",
        "                 bias=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        # 3 for 2sagcn 5 for stgcn\n",
        "        self.PA = nn.Parameter(torch.FloatTensor(3, adj_len, adj_len))\n",
        "        torch.nn.init.constant_(self.PA, 1e-6)\n",
        "\n",
        "        self.num_subset = 3\n",
        "        inter_channels = out_channels // 4\n",
        "        self.inter_c = inter_channels\n",
        "        self.conv_a = nn.ModuleList()\n",
        "        self.conv_b = nn.ModuleList()\n",
        "        self.conv_d = nn.ModuleList()\n",
        "        for i in range(self.num_subset):\n",
        "            self.conv_a.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_b.append(nn.Conv2d(in_channels, inter_channels, 1))\n",
        "            self.conv_d.append(nn.Conv2d(in_channels, out_channels, 1))\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.down = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        else:\n",
        "            self.down = lambda x: x\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.soft = nn.Softmax(-2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "        bn_init(self.bn, 1e-6)\n",
        "        for i in range(self.num_subset):\n",
        "            conv_branch_init(self.conv_d[i], self.num_subset)\n",
        "\n",
        "    def forward(self, x, adj_mat):\n",
        "        \"\"\"Defines the computation performed at every call.\"\"\"\n",
        "        assert adj_mat.size(0) == self.kernel_size\n",
        "\n",
        "        N, C, T, V = x.size()\n",
        "        A = adj_mat + self.PA\n",
        "\n",
        "        y = None\n",
        "        for i in range(self.num_subset):\n",
        "            A1 = self.conv_a[i](x).permute(0, 3, 1, 2).contiguous().view(\n",
        "                N, V, self.inter_c * T)\n",
        "            A2 = self.conv_b[i](x).view(N, self.inter_c * T, V)\n",
        "            A1 = self.soft(torch.matmul(A1, A2) / A1.size(-1))  # N V V\n",
        "            A1 = A1 + A[i]\n",
        "            A2 = x.view(N, C * T, V)\n",
        "            z = self.conv_d[i](torch.matmul(A2, A1).view(N, C, T, V))\n",
        "            y = z + y if y is not None else z\n",
        "        y = self.bn(y)\n",
        "        y += self.down(x)\n",
        "\n",
        "        return self.relu(y), adj_mat"
      ],
      "metadata": {
        "id": "DR5t2g9GVuFJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AGCNBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 stride=1,\n",
        "                 adj_len=25,\n",
        "                 dropout=0,\n",
        "                 residual=True):\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(kernel_size) == 2\n",
        "        assert kernel_size[0] % 2 == 1\n",
        "        padding = ((kernel_size[0] - 1) // 2, 0)\n",
        "\n",
        "        self.gcn = ConvTemporalGraphical(\n",
        "            in_channels, out_channels, kernel_size[1], adj_len=adj_len)\n",
        "        self.tcn = nn.Sequential(\n",
        "            nn.Conv2d(out_channels, out_channels, (kernel_size[0], 1),\n",
        "                      (stride, 1), padding), nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        # tcn init\n",
        "        for m in self.tcn.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                conv_init(m)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                bn_init(m, 1)\n",
        "\n",
        "        if not residual:\n",
        "            self.residual = zero\n",
        "\n",
        "        elif (in_channels == out_channels) and (stride == 1):\n",
        "            self.residual = identity\n",
        "\n",
        "        else:\n",
        "            self.residual = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=1,\n",
        "                    stride=(stride, 1)), nn.BatchNorm2d(out_channels))\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # put list of attention here ['pam', 'cam']\n",
        "        self.att_type = []\n",
        "        self.attention = len(self.att_type)\n",
        "        if self.attention > 0:\n",
        "          self.channel_attention = ChannelAttention(out_channels)\n",
        "\n",
        "    def forward(self, x, adj_mat):\n",
        "        \"\"\"Defines the computation performed at every call.\"\"\"\n",
        "        res = self.residual(x)\n",
        "        x, adj_mat = self.gcn(x, adj_mat)\n",
        "\n",
        "        if self.attention == 1 and self.att_type[0] == 'pam' :\n",
        "          x = self.channel_attention(x)\n",
        "          x = self.tcn(x)\n",
        "          x = x + res\n",
        "        elif self.attention == 1 and self.att_type[0] == 'cam':\n",
        "          x = self.tcn(x)\n",
        "          x = self.channel_attention(x)\n",
        "          x = x + res\n",
        "        else:\n",
        "          x = self.tcn(x) + res\n",
        "\n",
        "        if self.attention == 2:\n",
        "          x = self.channel_attention(x)\n",
        "\n",
        "        return self.relu(x), adj_mat"
      ],
      "metadata": {
        "id": "n1wZchWeVxpx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AGCN(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 hop_size,\n",
        "                 strategy,\n",
        "                 pretrained=None,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # load graph\n",
        "        self.graph = Graph(hop_size, strategy)\n",
        "        A = torch.tensor(\n",
        "            self.graph.A, dtype=torch.float32, requires_grad=False)\n",
        "        self.register_buffer('A', A)\n",
        "        A_size = A.size()\n",
        "\n",
        "        # build networks\n",
        "        spatial_kernel_size = A.size(0)\n",
        "        temporal_kernel_size = 9\n",
        "        kernel_size = (temporal_kernel_size, spatial_kernel_size)\n",
        "        self.bn = nn.BatchNorm1d(in_channels * A_size[1])\n",
        "\n",
        "\n",
        "        kwargs0 = {k: v for k, v in kwargs.items() if k != 'dropout'}\n",
        "        self.agcn_networks = nn.ModuleList((\n",
        "            AGCNBlock(\n",
        "                in_channels,\n",
        "                64,\n",
        "                kernel_size,\n",
        "                1,\n",
        "                adj_len=A.size(1),\n",
        "                residual=False,\n",
        "                **kwargs0),\n",
        "            AGCNBlock(64, 64, kernel_size, 1,adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(64, 64, kernel_size, 1, adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(64, 64, kernel_size, 1,adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(64, 128, kernel_size, 2, adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(128, 128, kernel_size, 1, adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(128, 128, kernel_size, 1, adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(128, 256, kernel_size, 2, adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(256, 256, kernel_size, 1, adj_len=A.size(1), **kwargs),\n",
        "            AGCNBlock(256, 256, kernel_size, 1, adj_len=A.size(1), **kwargs),\n",
        "        ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Defines the computation performed at every call.\n",
        "        Args:\n",
        "            x (torch.Tensor): The input data.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output of the module.\n",
        "        \"\"\"\n",
        "        # data normalization\n",
        "        N, C, T, V = x.size() # batch, channel, frame, node\n",
        "        x = x.permute(0, 3, 1, 2).contiguous().view(N, V * C, T)\n",
        "        x = self.bn(x)\n",
        "        x = x.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        for gcn in self.agcn_networks:\n",
        "            x, _ = gcn(x, self.A)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "xa-lF9ajV0sx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STGCN head"
      ],
      "metadata": {
        "id": "h0ayHWDLV5F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class STGCNHead(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 in_channels,\n",
        "                 loss_cls=dict(type='CrossEntropyLoss'),\n",
        "                 spatial_type='avg',\n",
        "                 num_person=1,\n",
        "                 init_std=0.01,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.spatial_type = spatial_type\n",
        "        self.in_channels = in_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.num_person = num_person\n",
        "        self.init_std = init_std\n",
        "\n",
        "        self.pool = None\n",
        "        if self.spatial_type == 'avg':\n",
        "            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        elif self.spatial_type == 'max':\n",
        "            self.pool = nn.AdaptiveMaxPool2d((1, 1))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.fc = nn.Conv2d(self.in_channels, self.num_classes, kernel_size=1)\n",
        "\n",
        "    def init_weights(self):\n",
        "        normal_init(self.fc, std=self.init_std)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # global pooling\n",
        "        assert self.pool is not None\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0] // self.num_person, self.num_person, -1, 1,\n",
        "                   1).mean(dim=1)\n",
        "\n",
        "        # prediction\n",
        "        x = self.fc(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "N8diQScXV_U0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Definition"
      ],
      "metadata": {
        "id": "WB1xuVLuWEMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##STGCN"
      ],
      "metadata": {
        "id": "vFqeSIJ6WK9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class STGCN(nn.Module):\n",
        "  def __init__(self,\n",
        "               num_classes,\n",
        "               in_channels,\n",
        "               t_kernel_size,\n",
        "               hop_size,\n",
        "               strategy='spatial'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.agcn = AGCN(in_channels, hop_size, strategy)\n",
        "    self.stgcnhead = STGCNHead(num_classes, in_channels=256)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.agcn(x)\n",
        "    x = self.stgcnhead(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "-XLHIqn4WF2y"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NTU Training"
      ],
      "metadata": {
        "id": "4xvXiw54Wkey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "class NTUCollator(object):\n",
        "    def __init__(self,test=False,percentile=4750):\n",
        "        self.test = test\n",
        "        self.percentile = percentile\n",
        "    def __call__(self, batch):\n",
        "        # data = [torch.Tensor(item[0]) for item in batch]\n",
        "\n",
        "\n",
        "\n",
        "        # data = pad_sequence(data, padding_value=0)\n",
        "        # target = pad_sequence(target, padding_value=0)\n",
        "        # target = torch.Tensor(target)\n",
        "        # for items in batch:\n",
        "        #   if items[0].shape[1]>300:\n",
        "        #     items[0]= items[0][:,:300,:,:]\n",
        "        max_len = 300\n",
        "        max_len2 = max([x[0].shape[2] for x in batch])\n",
        "\n",
        "        # return [data, target]\n",
        "        batch = sorted(batch, key=lambda x: x[0].shape[1], reverse=True)\n",
        "        target = [item[1] for item in batch]\n",
        "        data = [torch.Tensor(item[0]) for item in batch]\n",
        "        # Pad the sequences to the length of the longest sequence in the batch\n",
        "        # padded_batch = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "\n",
        "        padded_batch = [\n",
        "            torch.nn.functional.pad(\n",
        "                torch.Tensor(gif[0]),\n",
        "                (0, 0, max_len - gif[0].shape[1], 0),\n",
        "                value=0  # Specify the padding value if needed\n",
        "            )\n",
        "            for gif in batch\n",
        "        ]\n",
        "\n",
        "        # stack = [[seq[i],label[i]] for i in range(padded_batch.shape[0])]\n",
        "\n",
        "        return [torch.stack(padded_batch),target]\n",
        "\n",
        "\n",
        "class Feeder(torch.utils.data.Dataset):\n",
        "  def __init__(self, data_path, label_path):\n",
        "      super().__init__()\n",
        "      self.label = np.load(label_path)\n",
        "      self.data = np.load(data_path)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.label)\n",
        "\n",
        "  def __iter__(self):\n",
        "      return self\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      data = np.array(self.data[index])\n",
        "      label = self.label[index]\n",
        "\n",
        "      return data, label\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "NUM_EPOCH = 100\n",
        "BATCH_SIZE = 64\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "if(torch.cuda.is_available()):\n",
        "  model = STGCN(num_classes=99,\n",
        "                    in_channels=3,\n",
        "                    t_kernel_size=9,\n",
        "                    hop_size=1).cuda()\n",
        "else:\n",
        "  model = STGCN(num_classes=99,\n",
        "                    in_channels=3,\n",
        "                    t_kernel_size=9,\n",
        "                    hop_size=1)\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001, nesterov=True)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "data_loader = dict()\n",
        "collator = NTUCollator()\n",
        "data_loader['train'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='data/train_data.npy', label_path='data/train_label.npy'), batch_size=BATCH_SIZE, shuffle=True,collate_fn=collator)\n",
        "data_loader['test'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='data/test_data.npy', label_path='data/test_label.npy'), batch_size=BATCH_SIZE, shuffle=False,collate_fn=collator)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "  correct_train = 0\n",
        "  sum_loss = 0\n",
        "  model.train()\n",
        "  for batch_idx, (data, label) in enumerate(data_loader['train']):\n",
        "    # print(data)\n",
        "    if(torch.cuda.is_available()):\n",
        "      data = data.cuda()\n",
        "      label = torch.LongTensor(label).cuda()\n",
        "    else:\n",
        "      data = data\n",
        "      label = torch.LongTensor(label)\n",
        "    # data=data.reshape(-1,25,300,3)\n",
        "    output = model(data)\n",
        "    # print(len(output),len(label))\n",
        "    loss = criterion(output, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    sum_loss += loss.item()\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "    correct_train += (predict == label).sum().item()\n",
        "\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print('# Epoch: {} | Train Loss: {:.4f} | Train Accuracy: {:.4f} '.format(epoch, sum_loss/len(data_loader['train'].dataset),(100. * correct_train / len(data_loader['train'].dataset))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF2rwxndWnwi",
        "outputId": "725367c4-16ab-4cda-9832-1b779f62439d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Epoch: 1 | Train Loss: 0.0415 | Train Accuracy: 15.5500 \n",
            "# Epoch: 2 | Train Loss: 0.0344 | Train Accuracy: 20.8000 \n",
            "# Epoch: 3 | Train Loss: 0.0318 | Train Accuracy: 28.0000 \n",
            "# Epoch: 4 | Train Loss: 0.0254 | Train Accuracy: 42.6500 \n",
            "# Epoch: 5 | Train Loss: 0.0186 | Train Accuracy: 59.0000 \n",
            "# Epoch: 6 | Train Loss: 0.0167 | Train Accuracy: 61.3000 \n",
            "# Epoch: 7 | Train Loss: 0.0126 | Train Accuracy: 70.6000 \n",
            "# Epoch: 8 | Train Loss: 0.0118 | Train Accuracy: 74.5500 \n",
            "# Epoch: 9 | Train Loss: 0.0109 | Train Accuracy: 76.5000 \n",
            "# Epoch: 10 | Train Loss: 0.0091 | Train Accuracy: 80.5000 \n",
            "# Epoch: 11 | Train Loss: 0.0089 | Train Accuracy: 80.5500 \n",
            "# Epoch: 12 | Train Loss: 0.0078 | Train Accuracy: 83.8500 \n",
            "# Epoch: 13 | Train Loss: 0.0073 | Train Accuracy: 83.5000 \n",
            "# Epoch: 14 | Train Loss: 0.0063 | Train Accuracy: 86.2000 \n",
            "# Epoch: 15 | Train Loss: 0.0061 | Train Accuracy: 87.1500 \n",
            "# Epoch: 16 | Train Loss: 0.0056 | Train Accuracy: 87.3000 \n",
            "# Epoch: 17 | Train Loss: 0.0054 | Train Accuracy: 87.9000 \n",
            "# Epoch: 18 | Train Loss: 0.0050 | Train Accuracy: 88.4500 \n",
            "# Epoch: 19 | Train Loss: 0.0047 | Train Accuracy: 89.3000 \n",
            "# Epoch: 20 | Train Loss: 0.0049 | Train Accuracy: 88.9000 \n",
            "# Epoch: 21 | Train Loss: 0.0051 | Train Accuracy: 88.7500 \n",
            "# Epoch: 22 | Train Loss: 0.0047 | Train Accuracy: 89.8500 \n",
            "# Epoch: 23 | Train Loss: 0.0043 | Train Accuracy: 90.8000 \n",
            "# Epoch: 24 | Train Loss: 0.0052 | Train Accuracy: 89.2500 \n",
            "# Epoch: 25 | Train Loss: 0.0050 | Train Accuracy: 88.7000 \n",
            "# Epoch: 26 | Train Loss: 0.0040 | Train Accuracy: 90.5500 \n",
            "# Epoch: 27 | Train Loss: 0.0035 | Train Accuracy: 91.9000 \n",
            "# Epoch: 28 | Train Loss: 0.0035 | Train Accuracy: 92.0500 \n",
            "# Epoch: 29 | Train Loss: 0.0029 | Train Accuracy: 94.0000 \n",
            "# Epoch: 30 | Train Loss: 0.0033 | Train Accuracy: 92.7500 \n",
            "# Epoch: 31 | Train Loss: 0.0040 | Train Accuracy: 91.8500 \n",
            "# Epoch: 32 | Train Loss: 0.0036 | Train Accuracy: 92.1500 \n",
            "# Epoch: 33 | Train Loss: 0.0031 | Train Accuracy: 92.9000 \n",
            "# Epoch: 34 | Train Loss: 0.0024 | Train Accuracy: 94.7000 \n",
            "# Epoch: 35 | Train Loss: 0.0022 | Train Accuracy: 95.4500 \n",
            "# Epoch: 36 | Train Loss: 0.0025 | Train Accuracy: 94.3500 \n",
            "# Epoch: 37 | Train Loss: 0.0021 | Train Accuracy: 95.4500 \n",
            "# Epoch: 38 | Train Loss: 0.0026 | Train Accuracy: 94.1500 \n",
            "# Epoch: 39 | Train Loss: 0.0020 | Train Accuracy: 96.0500 \n",
            "# Epoch: 40 | Train Loss: 0.0019 | Train Accuracy: 95.5000 \n",
            "# Epoch: 41 | Train Loss: 0.0018 | Train Accuracy: 95.9500 \n",
            "# Epoch: 42 | Train Loss: 0.0020 | Train Accuracy: 95.9000 \n",
            "# Epoch: 43 | Train Loss: 0.0014 | Train Accuracy: 97.1000 \n",
            "# Epoch: 44 | Train Loss: 0.0020 | Train Accuracy: 96.0500 \n",
            "# Epoch: 45 | Train Loss: 0.0024 | Train Accuracy: 94.0500 \n",
            "# Epoch: 46 | Train Loss: 0.0018 | Train Accuracy: 96.5000 \n",
            "# Epoch: 47 | Train Loss: 0.0016 | Train Accuracy: 96.1000 \n",
            "# Epoch: 48 | Train Loss: 0.0017 | Train Accuracy: 96.4000 \n",
            "# Epoch: 49 | Train Loss: 0.0020 | Train Accuracy: 95.2000 \n",
            "# Epoch: 50 | Train Loss: 0.0015 | Train Accuracy: 96.5000 \n",
            "# Epoch: 51 | Train Loss: 0.0013 | Train Accuracy: 97.1500 \n",
            "# Epoch: 52 | Train Loss: 0.0009 | Train Accuracy: 98.3000 \n",
            "# Epoch: 53 | Train Loss: 0.0010 | Train Accuracy: 97.6000 \n",
            "# Epoch: 54 | Train Loss: 0.0012 | Train Accuracy: 97.6500 \n",
            "# Epoch: 55 | Train Loss: 0.0016 | Train Accuracy: 96.6500 \n",
            "# Epoch: 56 | Train Loss: 0.0015 | Train Accuracy: 97.2500 \n",
            "# Epoch: 57 | Train Loss: 0.0022 | Train Accuracy: 95.8500 \n",
            "# Epoch: 58 | Train Loss: 0.0018 | Train Accuracy: 95.9500 \n",
            "# Epoch: 59 | Train Loss: 0.0013 | Train Accuracy: 97.3500 \n",
            "# Epoch: 60 | Train Loss: 0.0011 | Train Accuracy: 97.6500 \n",
            "# Epoch: 61 | Train Loss: 0.0009 | Train Accuracy: 98.0500 \n",
            "# Epoch: 62 | Train Loss: 0.0009 | Train Accuracy: 98.2500 \n",
            "# Epoch: 63 | Train Loss: 0.0014 | Train Accuracy: 97.1500 \n",
            "# Epoch: 64 | Train Loss: 0.0013 | Train Accuracy: 96.9500 \n",
            "# Epoch: 65 | Train Loss: 0.0008 | Train Accuracy: 98.4500 \n",
            "# Epoch: 66 | Train Loss: 0.0007 | Train Accuracy: 98.4000 \n",
            "# Epoch: 67 | Train Loss: 0.0009 | Train Accuracy: 97.9500 \n",
            "# Epoch: 68 | Train Loss: 0.0005 | Train Accuracy: 98.9000 \n",
            "# Epoch: 69 | Train Loss: 0.0003 | Train Accuracy: 99.5000 \n",
            "# Epoch: 70 | Train Loss: 0.0004 | Train Accuracy: 99.3500 \n",
            "# Epoch: 71 | Train Loss: 0.0008 | Train Accuracy: 98.7000 \n",
            "# Epoch: 72 | Train Loss: 0.0007 | Train Accuracy: 98.5000 \n",
            "# Epoch: 73 | Train Loss: 0.0004 | Train Accuracy: 99.0000 \n",
            "# Epoch: 74 | Train Loss: 0.0004 | Train Accuracy: 99.4000 \n",
            "# Epoch: 75 | Train Loss: 0.0006 | Train Accuracy: 98.7000 \n",
            "# Epoch: 76 | Train Loss: 0.0004 | Train Accuracy: 99.3000 \n",
            "# Epoch: 77 | Train Loss: 0.0010 | Train Accuracy: 98.3500 \n",
            "# Epoch: 78 | Train Loss: 0.0015 | Train Accuracy: 96.7500 \n",
            "# Epoch: 79 | Train Loss: 0.0006 | Train Accuracy: 98.9000 \n",
            "# Epoch: 80 | Train Loss: 0.0003 | Train Accuracy: 99.5000 \n",
            "# Epoch: 81 | Train Loss: 0.0003 | Train Accuracy: 99.5500 \n",
            "# Epoch: 82 | Train Loss: 0.0004 | Train Accuracy: 99.1500 \n",
            "# Epoch: 83 | Train Loss: 0.0009 | Train Accuracy: 98.6500 \n",
            "# Epoch: 84 | Train Loss: 0.0015 | Train Accuracy: 97.2500 \n",
            "# Epoch: 85 | Train Loss: 0.0008 | Train Accuracy: 98.2000 \n",
            "# Epoch: 86 | Train Loss: 0.0006 | Train Accuracy: 98.8500 \n",
            "# Epoch: 87 | Train Loss: 0.0006 | Train Accuracy: 99.0500 \n",
            "# Epoch: 88 | Train Loss: 0.0003 | Train Accuracy: 99.4500 \n",
            "# Epoch: 89 | Train Loss: 0.0003 | Train Accuracy: 99.5000 \n",
            "# Epoch: 90 | Train Loss: 0.0003 | Train Accuracy: 99.5500 \n",
            "# Epoch: 91 | Train Loss: 0.0001 | Train Accuracy: 99.9500 \n",
            "# Epoch: 92 | Train Loss: 0.0002 | Train Accuracy: 99.6500 \n",
            "# Epoch: 93 | Train Loss: 0.0001 | Train Accuracy: 99.9000 \n",
            "# Epoch: 94 | Train Loss: 0.0001 | Train Accuracy: 99.9000 \n",
            "# Epoch: 95 | Train Loss: 0.0002 | Train Accuracy: 99.8500 \n",
            "# Epoch: 96 | Train Loss: 0.0005 | Train Accuracy: 99.1000 \n",
            "# Epoch: 97 | Train Loss: 0.0005 | Train Accuracy: 98.9000 \n",
            "# Epoch: 98 | Train Loss: 0.0004 | Train Accuracy: 99.4000 \n",
            "# Epoch: 99 | Train Loss: 0.0002 | Train Accuracy: 99.7500 \n",
            "# Epoch: 100 | Train Loss: 0.0004 | Train Accuracy: 99.5500 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'ntu_weights.pth')"
      ],
      "metadata": {
        "id": "x8H-nuAUWxDi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Loop"
      ],
      "metadata": {
        "id": "D7FH_DLvWzfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "USE_WEIGHTS = True\n",
        "NUM_EPOCH = 80\n",
        "BATCH_SIZE = 32\n",
        "CUDA_LAUNCH_BLOCKING=1\n",
        "if(torch.cuda.is_available()):\n",
        "  model = STGCN(num_classes=99,\n",
        "                    in_channels=3,\n",
        "                    t_kernel_size=9,\n",
        "                    hop_size=1).cuda()\n",
        "else:\n",
        "  model = STGCN(num_classes=99,\n",
        "                    in_channels=3,\n",
        "                    t_kernel_size=9,\n",
        "                    hop_size=1)\n",
        "\n",
        "if USE_WEIGHTS:\n",
        "  model.load_state_dict(torch.load('/content/ntu_weights.pth'))\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001, nesterov=True)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "data_loader = dict()\n",
        "collator = MyCollator()\n",
        "data_loader['train'] = torch.utils.data.DataLoader(dataset=Feeder(data=train_data, label=train_label), batch_size=BATCH_SIZE, shuffle=True,collate_fn=collator)\n",
        "data_loader['valid'] = torch.utils.data.DataLoader(dataset=Feeder(data=val_data, label=val_label), batch_size=BATCH_SIZE, shuffle=True,collate_fn=collator)\n",
        "data_loader['test'] = torch.utils.data.DataLoader(dataset=Feeder(data=test_data, label=test_label), batch_size=BATCH_SIZE, shuffle=False,collate_fn=collator)\n",
        "\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1, NUM_EPOCH+1):\n",
        "  correct_train = 0\n",
        "  sum_loss = 0\n",
        "  model.train()\n",
        "  for batch_idx, (data, label) in enumerate(data_loader['train']):\n",
        "    # print(data)\n",
        "    if(torch.cuda.is_available()):\n",
        "      data = data.cuda()\n",
        "      label = torch.LongTensor(label).cuda()\n",
        "    else:\n",
        "      data = data\n",
        "      label = torch.LongTensor(label)\n",
        "\n",
        "\n",
        "    output = model(data)\n",
        "    # print(len(output),len(label))\n",
        "    loss = criterion(output, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    sum_loss += loss.item()\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "    correct_train += (predict == label).sum().item()\n",
        "\n",
        "  model.eval()\n",
        "  val_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data, label in data_loader['valid']:\n",
        "          if(torch.cuda.is_available()):\n",
        "            data = data.cuda()\n",
        "            label = torch.LongTensor(label).cuda()\n",
        "          else:\n",
        "            data = data\n",
        "            label = torch.LongTensor(label)\n",
        "          outputs = model(data)\n",
        "          val_loss =criterion(outputs, label)\n",
        "\n",
        "          sum_loss += val_loss.item()\n",
        "          _, predict = torch.max(outputs.data, 1)\n",
        "          correct += (predict == label).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "  print('# Epoch: {} | Train Loss: {:.4f} | Train Accuracy: {:.4f} | Val loss: {:.4f} | Val Accuracy: {:.4f}'.format(epoch, sum_loss/len(data_loader['train'].dataset),(100. * correct_train / len(data_loader['train'].dataset)),val_loss/len(data_loader['valid'].dataset), (100. * correct / len(data_loader['valid'].dataset))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njhASaqFW26y",
        "outputId": "ffbf89ea-4f22-41d5-dba4-f4e87ab9ef22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Epoch: 1 | Train Loss: 0.1531 | Train Accuracy: 3.6966 | Val loss: 0.0034 | Val Accuracy: 8.1294\n",
            "# Epoch: 2 | Train Loss: 0.1272 | Train Accuracy: 10.5150 | Val loss: 0.0029 | Val Accuracy: 8.0505\n",
            "# Epoch: 3 | Train Loss: 0.1175 | Train Accuracy: 13.9863 | Val loss: 0.0025 | Val Accuracy: 17.9163\n",
            "# Epoch: 4 | Train Loss: 0.1093 | Train Accuracy: 19.7678 | Val loss: 0.0022 | Val Accuracy: 13.4964\n",
            "# Epoch: 5 | Train Loss: 0.1011 | Train Accuracy: 24.8281 | Val loss: 0.0026 | Val Accuracy: 15.7853\n",
            "# Epoch: 6 | Train Loss: 0.0955 | Train Accuracy: 31.5226 | Val loss: 0.0034 | Val Accuracy: 19.6527\n",
            "# Epoch: 7 | Train Loss: 0.0783 | Train Accuracy: 39.6033 | Val loss: 0.0018 | Val Accuracy: 36.5430\n",
            "# Epoch: 8 | Train Loss: 0.0671 | Train Accuracy: 48.8674 | Val loss: 0.0019 | Val Accuracy: 40.9629\n",
            "# Epoch: 9 | Train Loss: 0.0541 | Train Accuracy: 56.2155 | Val loss: 0.0011 | Val Accuracy: 54.2226\n",
            "# Epoch: 10 | Train Loss: 0.0573 | Train Accuracy: 62.2112 | Val loss: 0.0023 | Val Accuracy: 25.7301\n",
            "# Epoch: 11 | Train Loss: 0.0488 | Train Accuracy: 67.8125 | Val loss: 0.0028 | Val Accuracy: 30.4657\n",
            "# Epoch: 12 | Train Loss: 0.0411 | Train Accuracy: 72.9178 | Val loss: 0.0037 | Val Accuracy: 38.5162\n",
            "# Epoch: 13 | Train Loss: 0.0269 | Train Accuracy: 78.7220 | Val loss: 0.0007 | Val Accuracy: 68.9818\n",
            "# Epoch: 14 | Train Loss: 0.0200 | Train Accuracy: 83.7034 | Val loss: 0.0009 | Val Accuracy: 80.3473\n",
            "# Epoch: 15 | Train Loss: 0.0187 | Train Accuracy: 86.6900 | Val loss: 0.0011 | Val Accuracy: 71.2707\n",
            "# Epoch: 16 | Train Loss: 0.0137 | Train Accuracy: 87.8959 | Val loss: 0.0009 | Val Accuracy: 89.1871\n",
            "# Epoch: 17 | Train Loss: 0.0147 | Train Accuracy: 90.0823 | Val loss: 0.0014 | Val Accuracy: 70.5604\n",
            "# Epoch: 18 | Train Loss: 0.0104 | Train Accuracy: 91.9757 | Val loss: 0.0001 | Val Accuracy: 87.0560\n",
            "# Epoch: 19 | Train Loss: 0.0104 | Train Accuracy: 93.9930 | Val loss: 0.0002 | Val Accuracy: 77.8216\n",
            "# Epoch: 20 | Train Loss: 0.0071 | Train Accuracy: 94.2184 | Val loss: 0.0001 | Val Accuracy: 93.2123\n",
            "# Epoch: 21 | Train Loss: 0.0074 | Train Accuracy: 93.8916 | Val loss: 0.0001 | Val Accuracy: 91.8706\n",
            "# Epoch: 22 | Train Loss: 0.0058 | Train Accuracy: 95.2440 | Val loss: 0.0007 | Val Accuracy: 93.3702\n",
            "# Epoch: 23 | Train Loss: 0.0052 | Train Accuracy: 96.2358 | Val loss: 0.0000 | Val Accuracy: 93.2123\n",
            "# Epoch: 24 | Train Loss: 0.0048 | Train Accuracy: 96.6528 | Val loss: 0.0003 | Val Accuracy: 92.5809\n",
            "# Epoch: 25 | Train Loss: 0.0109 | Train Accuracy: 97.2501 | Val loss: 0.0016 | Val Accuracy: 62.5888\n",
            "# Epoch: 26 | Train Loss: 0.0042 | Train Accuracy: 96.7091 | Val loss: 0.0000 | Val Accuracy: 95.4223\n",
            "# Epoch: 27 | Train Loss: 0.0034 | Train Accuracy: 97.6107 | Val loss: 0.0001 | Val Accuracy: 94.7119\n",
            "# Epoch: 28 | Train Loss: 0.0023 | Train Accuracy: 98.5912 | Val loss: 0.0001 | Val Accuracy: 95.7380\n",
            "# Epoch: 29 | Train Loss: 0.0043 | Train Accuracy: 98.4334 | Val loss: 0.0002 | Val Accuracy: 87.3717\n",
            "# Epoch: 30 | Train Loss: 0.0042 | Train Accuracy: 98.2306 | Val loss: 0.0001 | Val Accuracy: 87.6085\n",
            "# Epoch: 31 | Train Loss: 0.0052 | Train Accuracy: 96.3147 | Val loss: 0.0001 | Val Accuracy: 92.5809\n",
            "# Epoch: 32 | Train Loss: 0.0044 | Train Accuracy: 96.6753 | Val loss: 0.0005 | Val Accuracy: 95.6590\n",
            "# Epoch: 33 | Train Loss: 0.0039 | Train Accuracy: 97.8249 | Val loss: 0.0002 | Val Accuracy: 89.8185\n",
            "# Epoch: 34 | Train Loss: 0.0030 | Train Accuracy: 97.5769 | Val loss: 0.0000 | Val Accuracy: 97.2376\n",
            "# Epoch: 35 | Train Loss: 0.0028 | Train Accuracy: 99.0871 | Val loss: 0.0003 | Val Accuracy: 91.2391\n",
            "# Epoch: 36 | Train Loss: 0.0019 | Train Accuracy: 98.7828 | Val loss: 0.0000 | Val Accuracy: 97.4743\n",
            "# Epoch: 37 | Train Loss: 0.0033 | Train Accuracy: 98.0052 | Val loss: 0.0001 | Val Accuracy: 92.2652\n",
            "# Epoch: 38 | Train Loss: 0.0027 | Train Accuracy: 98.3658 | Val loss: 0.0003 | Val Accuracy: 96.6062\n",
            "# Epoch: 39 | Train Loss: 0.0017 | Train Accuracy: 99.0533 | Val loss: 0.0000 | Val Accuracy: 97.3954\n",
            "# Epoch: 40 | Train Loss: 0.0014 | Train Accuracy: 99.2900 | Val loss: 0.0000 | Val Accuracy: 97.5533\n",
            "# Epoch: 41 | Train Loss: 0.0009 | Train Accuracy: 99.6506 | Val loss: 0.0002 | Val Accuracy: 97.6322\n",
            "# Epoch: 42 | Train Loss: 0.0016 | Train Accuracy: 99.7971 | Val loss: 0.0002 | Val Accuracy: 92.8177\n",
            "# Epoch: 43 | Train Loss: 0.0033 | Train Accuracy: 98.0728 | Val loss: 0.0004 | Val Accuracy: 93.8437\n",
            "# Epoch: 44 | Train Loss: 0.0041 | Train Accuracy: 97.1487 | Val loss: 0.0004 | Val Accuracy: 95.6590\n",
            "# Epoch: 45 | Train Loss: 0.0038 | Train Accuracy: 98.1292 | Val loss: 0.0003 | Val Accuracy: 88.8713\n",
            "# Epoch: 46 | Train Loss: 0.0058 | Train Accuracy: 95.2553 | Val loss: 0.0000 | Val Accuracy: 95.3433\n",
            "# Epoch: 47 | Train Loss: 0.0038 | Train Accuracy: 97.4191 | Val loss: 0.0000 | Val Accuracy: 94.1594\n",
            "# Epoch: 48 | Train Loss: 0.0031 | Train Accuracy: 97.8361 | Val loss: 0.0000 | Val Accuracy: 95.7380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation of Model"
      ],
      "metadata": {
        "id": "WQoBFFZhXIK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "def prepare_gif_for_model(gif_path,length=300):\n",
        "  gif = imageio.mimread(gif_path)\n",
        "  resized_frames = []\n",
        "  for frame in gif:\n",
        "      image = Image.fromarray(frame)\n",
        "      resized_image = image.resize((25, 25))\n",
        "      resized_frames.append(np.array(resized_image))\n",
        "  frames = np.asarray(resized_frames)\n",
        "  frames = np.transpose(frames, (3, 0, 1, 2))\n",
        "\n",
        "\n",
        "  frames=frames.reshape((3,-1,25)) #check this\n",
        "  if frames.shape[1]>length:\n",
        "      frames= frames[:,:length,:]\n",
        "  data = torch.Tensor(frames)\n",
        "\n",
        "  return np.array(data)"
      ],
      "metadata": {
        "id": "MbTXv9XFXN0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1].shape"
      ],
      "metadata": {
        "id": "9DqOaAFzXP6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test one file"
      ],
      "metadata": {
        "id": "XpjFcyUxXSZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instance=prepare_gif_for_model(\"/content/205_18_0_1_1_stand.txt.gif\")"
      ],
      "metadata": {
        "id": "jBUmC17gXXaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    if(torch.cuda.is_available()):\n",
        "        data = torch.tensor([instance]).cuda()\n",
        "    else:\n",
        "        data = torch.tensor([instance])\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "\n",
        "print(predict)"
      ],
      "metadata": {
        "id": "MawF8ce9Xzti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##test pack of files"
      ],
      "metadata": {
        "id": "IYWA--f1X2R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "confusion_matrix = np.zeros((100, 100))\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_idx, (data, label) in enumerate(data_loader['test']):\n",
        "    if(torch.cuda.is_available()):\n",
        "      data = data.cuda()\n",
        "      label = torch.LongTensor(label).cuda()\n",
        "    else:\n",
        "      data = data\n",
        "      label = torch.LongTensor(label)\n",
        "\n",
        "    output = model(data)\n",
        "\n",
        "    _, predict = torch.max(output.data, 1)\n",
        "    correct += (predict == label).sum().item()\n",
        "\n",
        "    for l, p in zip(label.view(-1), predict.view(-1)):\n",
        "      confusion_matrix[l.long(), p.long()] += 1\n",
        "\n",
        "len_cm = len(confusion_matrix)\n",
        "for i in range(len_cm):\n",
        "    sum_cm = np.sum(confusion_matrix[i])\n",
        "    for j in range(len_cm):\n",
        "        confusion_matrix[i][j] = 100 * (confusion_matrix[i][j] / sum_cm)\n",
        "\n",
        "classes = np.unique(train_label,return_counts=False)\n",
        "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion matrix')\n",
        "plt.tight_layout()\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes, rotation=45)\n",
        "plt.yticks(tick_marks, classes)\n",
        "plt.ylabel('True')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('# Test Accuracy: {:.3f}[%]'.format(100. * correct / len(data_loader['test'].dataset)))"
      ],
      "metadata": {
        "id": "l8PTFvCfX7sK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}